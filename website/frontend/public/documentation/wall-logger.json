{
  "id": "wall-logger",
  "title": "Wall Logger - Comprehensive Logging",
  "description": "Automatic logging of all Wall Library operations",
  "content": [
    {
      "type": "text",
      "content": "**Wall Logger** provides automatic logging of all Wall Library operations. It logs validation operations, RAG retrievals, scoring operations, LLM calls, and monitoring events, making debugging and compliance easy."
    },
    {
      "type": "section",
      "title": "Understanding Wall Logger",
      "subsections": [
        {
          "title": "What is Wall Logger?",
          "type": "text",
          "content": "Wall Logger is a comprehensive logging system that automatically logs all Wall Library operations. Unlike traditional logging that requires manual log statements, Wall Logger automatically captures:\n\n- **Validation operations**: Every validation pass/fail with details\n- **RAG retrievals**: Queries, retrieved documents, scores\n- **Scoring operations**: Metrics, scores, aggregated results\n- **LLM calls**: Inputs, outputs, latency\n- **Monitoring events**: All tracked interactions\n\nThis automatic logging makes debugging and compliance much easier - you don't need to remember to add log statements everywhere."
        },
        {
          "title": "Why Use Wall Logger?",
          "type": "list",
          "items": [
            "**Automatic Logging**: Set logger once, all operations are logged automatically",
            "**Multiple Formats**: JSON (for parsing) and human-readable (for reading)",
            "**Multiple Outputs**: File (persistent) and console (real-time)",
            "**Configurable Scopes**: Log only what you need (VALIDATION, RAG, SCORING, etc.)",
            "**Compliance**: Maintain audit trail for regulatory requirements",
            "**Debugging**: Trace issues with detailed operation logs",
            "**Integration**: Works with log aggregators (ELK, Splunk, etc.)"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "Log Scopes - What Gets Logged",
      "subsections": [
        {
          "title": "Available Scopes",
          "type": "text",
          "content": "Log scopes control what operations are logged. You can log everything or be selective:\n\n- **VALIDATION**: Log all validation operations (pass/fail, errors, validators used)\n- **RAG**: Log RAG retrievals (queries, retrieved docs, scores, metadata)\n- **SCORING**: Log scoring operations (metrics, scores, aggregated results)\n- **LLM_CALLS**: Log LLM interactions (inputs, outputs, latency, metadata)\n- **MONITORING**: Log monitoring events (tracked interactions)\n- **ALL**: Log everything (recommended for production)"
        },
        {
          "title": "Scope Examples",
          "type": "code",
          "code": "from wall_library.logging import WallLogger, LogScope\n\n# Log only validation operations\nlogger = WallLogger(\n    scopes=[LogScope.VALIDATION.value],\n    output=\"file\",\n    log_file=\"validation.log\"\n)\n\n# Log validation and RAG\nlogger = WallLogger(\n    scopes=[LogScope.VALIDATION.value, LogScope.RAG.value],\n    output=\"both\",\n    log_file=\"validation_rag.log\"\n)\n\n# Log everything (recommended for production)\nlogger = WallLogger(\n    scopes=[LogScope.ALL.value],\n    output=\"file\",\n    log_file=\"wall_library.log\"\n)",
          "input": "Configuring scopes...",
          "output": "✅ Logger configured with specified scopes"
        }
      ]
    },
    {
      "type": "section",
      "title": "Complete Setup Guide",
      "subsections": [
        {
          "title": "Step 1: Import and Create Logger",
          "type": "code",
          "code": "from wall_library.logging import WallLogger, LogScope\nimport os\n\n# Create log directory\nlog_dir = os.path.join(os.getcwd(), \"logs\")\nos.makedirs(log_dir, exist_ok=True)\nlog_file = os.path.join(log_dir, \"wall_library.log\")\n\n# Create logger\nlogger = WallLogger(\n    level=\"INFO\",  # DEBUG, INFO, WARNING, ERROR\n    scopes=[LogScope.ALL.value],  # Log everything\n    output=\"both\",  # \"console\", \"file\", or \"both\"\n    format=\"both\",  # \"json\", \"human\", or \"both\"\n    log_file=log_file,\n    max_bytes=10 * 1024 * 1024,  # 10MB before rotation\n    backup_count=5  # Keep 5 backup files\n)",
          "input": "Creating logger...",
          "output": "✅ Logger created"
        },
        {
          "title": "Step 2: Set Logger on Components",
          "type": "code",
          "code": "from wall_library import WallGuard\nfrom wall_library.rag import RAGRetriever\nfrom wall_library.scoring import ResponseScorer\nfrom wall_library.monitoring import LLMMonitor\n\n# Set logger on all components\n# Once set, all operations are automatically logged\n\nguard = WallGuard()\nguard.set_logger(logger)  # All validations will be logged\n\nrag_retriever = RAGRetriever(...)\nrag_retriever.set_logger(logger)  # All RAG retrievals will be logged\n\nscorer = ResponseScorer()\nscorer.set_logger(logger)  # All scoring operations will be logged\n\nmonitor = LLMMonitor()\nmonitor.set_logger(logger)  # All LLM calls will be logged\n\n# Now all operations are automatically logged!",
          "input": "Setting logger on components...",
          "output": "✅ Logger set on all components"
        },
        {
          "title": "Step 3: Operations Are Automatically Logged",
          "type": "code",
          "code": "# Now when you use components, everything is logged automatically\n\n# Validation is logged\nguard.validate(\"Some text\")\n# Log entry: Validation operation with result, validator name, etc.\n\n# RAG retrieval is logged\nrag_retriever.retrieve(\"What are diabetes symptoms?\")\n# Log entry: Query, retrieved documents, scores, metadata\n\n# Scoring is logged\nscorer.score(response, expected)\n# Log entry: Metrics, scores, aggregated result\n\n# LLM calls are logged\nmonitor.track_call(input_data, output, metadata, latency)\n# Log entry: Input, output, latency, metadata\n\n# No need to manually add log statements!",
          "input": "Operations running...",
          "output": "✅ All operations automatically logged"
        }
      ]
    },
    {
      "type": "section",
      "title": "Output Formats",
      "subsections": [
        {
          "title": "JSON Format",
          "type": "text",
          "content": "JSON format is structured and machine-readable. It's perfect for:\n\n- **Log Aggregators**: ELK stack, Splunk, Datadog, etc.\n- **Parsing**: Easy to parse and analyze programmatically\n- **Searching**: Fast searching in log aggregators\n- **Integration**: Works with monitoring and alerting systems\n\nExample JSON log entry:\n```json\n{\n  \"timestamp\": \"2024-01-08T10:30:45.123Z\",\n  \"level\": \"INFO\",\n  \"scope\": \"VALIDATION\",\n  \"message\": \"Validation operation\",\n  \"metadata\": {\n    \"validator_name\": \"SafetyValidator\",\n    \"result\": \"PASS\",\n    \"value\": \"Common symptoms of diabetes...\"\n  }\n}\n```"
        },
        {
          "title": "Human-Readable Format",
          "type": "text",
          "content": "Human-readable format is easy to read and understand. It's perfect for:\n\n- **Development**: Easy to read during development\n- **Debugging**: Quick inspection of logs\n- **Console Output**: Real-time monitoring\n\nExample human-readable log entry:\n```\n[2024-01-08 10:30:45] INFO [VALIDATION] Validation operation\n  Validator: SafetyValidator\n  Result: PASS\n  Value: Common symptoms of diabetes...\n```"
        },
        {
          "title": "Both Formats",
          "type": "text",
          "content": "Using both formats gives you the best of both worlds:\n\n- **File**: Store both JSON and human-readable in file\n- **Console**: Show human-readable for real-time monitoring\n- **Analytics**: Parse JSON for analysis\n- **Debugging**: Read human-readable for quick inspection"
        }
      ]
    },
    {
      "type": "section",
      "title": "Output Destinations",
      "subsections": [
        {
          "title": "File Output",
          "type": "text",
          "content": "File output provides persistent storage:\n\n- **Persistence**: Logs survive application restarts\n- **Rotation**: Automatic log rotation (configurable size and backup count)\n- **Storage**: Store logs for compliance and analysis\n- **Analysis**: Analyze historical logs\n\n**Use when**: Production, compliance, long-term storage"
        },
        {
          "title": "Console Output",
          "type": "text",
          "content": "Console output provides real-time monitoring:\n\n- **Real-time**: See logs as they happen\n- **Development**: Useful during development\n- **Debugging**: Quick inspection of current operations\n- **No Storage**: Not persisted (lost when application stops)\n\n**Use when**: Development, debugging, real-time monitoring"
        },
        {
          "title": "Both Outputs",
          "type": "text",
          "content": "Using both outputs gives you:\n\n- **Real-time Monitoring**: See logs in console\n- **Persistent Storage**: Store logs in file\n- **Best of Both**: Real-time visibility + long-term storage\n\n**Use when**: Production (recommended)"
        }
      ]
    },
    {
      "type": "section",
      "title": "Complete Example",
      "subsections": [
        {
          "title": "Production-Ready Setup",
          "type": "code",
          "code": "from wall_library import WallGuard, OnFailAction, WallLogger, LogScope\nfrom wall_library.rag import RAGRetriever, ChromaDBClient\nfrom wall_library.scoring import ResponseScorer\nfrom wall_library.monitoring import LLMMonitor\nimport os\nfrom datetime import datetime\n\n# Setup logger with production settings\nlog_dir = os.path.join(os.getcwd(), \"logs\")\nos.makedirs(log_dir, exist_ok=True)\n\n# Use timestamped log file\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nlog_file = os.path.join(log_dir, f\"wall_library_{timestamp}.log\")\n\nlogger = WallLogger(\n    level=\"INFO\",  # INFO level for production (not too verbose)\n    scopes=[LogScope.ALL.value],  # Log everything\n    output=\"both\",  # File + console\n    format=\"both\",  # JSON + human-readable\n    log_file=log_file,\n    max_bytes=50 * 1024 * 1024,  # 50MB before rotation\n    backup_count=10  # Keep 10 backup files\n)\n\n# Setup all components with logger\nguard = WallGuard().use((SafetyValidator, {}, OnFailAction.EXCEPTION))\nguard.set_logger(logger)\n\nrag_retriever = RAGRetriever(chromadb_client=ChromaDBClient())\nrag_retriever.set_logger(logger)\n\nscorer = ResponseScorer()\nscorer.set_logger(logger)\n\nmonitor = LLMMonitor()\nmonitor.set_logger(logger)\n\n# Now all operations are automatically logged:\n# - Every validation (pass/fail, errors)\n# - Every RAG retrieval (query, results, scores)\n# - Every scoring operation (metrics, scores)\n# - Every LLM call (input, output, latency)\n# - Every monitoring event\n\n# Use components normally - logging happens automatically\nresult = guard.validate(\"Some text\")\nretrieved = rag_retriever.retrieve(\"Query\")\nscores = scorer.score(response, expected)\nmonitor.track_call(input_data, output, metadata, latency)",
          "input": "Setting up production logging...",
          "output": "✅ Production logging configured. All operations will be logged."
        }
      ]
    },
    {
      "type": "section",
      "title": "Log Levels",
      "subsections": [
        {
          "title": "Understanding Log Levels",
          "type": "text",
          "content": "Log levels control verbosity:\n\n- **DEBUG**: Very detailed information (development only)\n- **INFO**: General information (production recommended)\n- **WARNING**: Warning messages (potential issues)\n- **ERROR**: Error messages (actual problems)\n\n**Recommendation**: Use INFO for production (good balance of detail and verbosity)"
        },
        {
          "title": "Level Examples",
          "type": "code",
          "code": "# Development: Use DEBUG for maximum detail\nlogger = WallLogger(level=\"DEBUG\", ...)\n\n# Production: Use INFO for balanced detail\nlogger = WallLogger(level=\"INFO\", ...)\n\n# Minimal: Use WARNING to only log warnings and errors\nlogger = WallLogger(level=\"WARNING\", ...)",
          "input": "Configuring log levels...",
          "output": "✅ Log level configured"
        }
      ]
    },
    {
      "type": "section",
      "title": "Best Practices",
      "subsections": [
        {
          "title": "Recommendations",
          "type": "list",
          "items": [
            "**Use LogScope.ALL in production**: Log everything for complete audit trail",
            "**Use both formats**: JSON for parsing, human-readable for reading",
            "**Use both outputs**: File for persistence, console for real-time",
            "**Set up log rotation**: Configure max_bytes and backup_count appropriately",
            "**Use INFO level in production**: Good balance of detail and verbosity",
            "**Set logger on all components**: Don't miss any operations",
            "**Integrate with log aggregators**: Export logs to ELK, Splunk, etc. for analysis",
            "**Use timestamped log files**: Easier to manage and analyze"
          ]
        },
        {
          "title": "Common Mistakes",
          "type": "list",
          "items": [
            "❌ Not setting logger on components (operations not logged)",
            "❌ Using DEBUG level in production (too verbose, performance impact)",
            "❌ Not setting up log rotation (files grow too large)",
            "❌ Only using console output (logs lost on restart)",
            "❌ Not using JSON format (hard to parse and analyze)",
            "❌ Logging too little (missing important information)",
            "❌ Not integrating with log aggregators (hard to analyze at scale)"
          ]
        }
      ]
    }
  ]
}
