{
  "id": "streaming-async",
  "title": "Streaming & Async Support",
  "description": "Full support for streaming and asynchronous operations",
  "content": [
    {
      "type": "text",
      "content": "Wall Library supports both streaming and asynchronous operations, allowing you to build modern, responsive applications that handle LLM interactions efficiently."
    },
    {
      "type": "section",
      "title": "Understanding Streaming & Async",
      "subsections": [
        {
          "title": "What is Streaming?",
          "type": "text",
          "content": "Streaming processes LLM responses chunk-by-chunk as they arrive, rather than waiting for the complete response.\n\n**Benefits**:\n- **Faster perceived response**: Users see output immediately\n- **Better UX**: Real-time feedback\n- **Lower latency**: Start processing before response completes\n- **Memory efficient**: Process chunks without storing full response\n\n**Use when**: Building user-facing applications where you want to show responses as they're generated."
        },
        {
          "title": "What is Async?",
          "type": "text",
          "content": "Async (asynchronous) operations are non-blocking, allowing your application to handle multiple requests concurrently.\n\n**Benefits**:\n- **Concurrency**: Handle multiple requests simultaneously\n- **Non-blocking**: Don't block while waiting for LLM\n- **Scalability**: Handle more requests with same resources\n- **Responsiveness**: Application stays responsive\n\n**Use when**: Building web applications, APIs, or any application that needs to handle multiple requests."
        }
      ]
    },
    {
      "type": "section",
      "title": "Streaming - Complete Guide",
      "subsections": [
        {
          "title": "Step 1: Create Streaming LLM API",
          "type": "code",
          "code": "from openai import OpenAI\n\nclient = OpenAI(api_key=\"your-api-key\")\n\ndef llm_api_stream(prompt: str, **kwargs):\n    \"\"\"Streaming LLM API that yields chunks.\"\"\"\n    stream = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        stream=True,  # Enable streaming\n        **kwargs\n    )\n    \n    # Yield chunks as they arrive\n    for chunk in stream:\n        if chunk.choices[0].delta.content:\n            yield chunk.choices[0].delta.content",
          "input": "Creating streaming API...",
          "output": "✅ Streaming API created"
        },
        {
          "title": "Step 2: Use StreamRunner",
          "type": "code",
          "code": "from wall_library.run import StreamRunner\nfrom wall_library import WallGuard\n\n# Create guard (as usual)\nguard = WallGuard().use((SafetyValidator, {}, OnFailAction.EXCEPTION))\n\n# Create stream runner\nstream_runner = StreamRunner(\n    api=llm_api_stream,\n    validation_map=guard.validator_map  # Use guard's validators\n)\n\n# Stream validated chunks\nquery = \"What are diabetes symptoms?\"\nfor chunk in stream_runner.stream(query):\n    # Each chunk is validated before being yielded\n    print(chunk, end=\"\", flush=True)  # Print without newline, flush immediately\n\n# Output appears in real-time, chunk by chunk\n# Each chunk is validated before being shown to user",
          "input": "Streaming response...",
          "output": "Common symptoms of diabetes include increased thirst and frequent urination..."
        },
        {
          "title": "Step 3: Handle Streaming in Web Application",
          "type": "code",
          "code": "from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\n\napp = FastAPI()\n\n@app.get(\"/stream\")\nasync def stream_response(query: str):\n    \"\"\"Stream validated LLM response.\"\"\"\n    \n    def generate():\n        for chunk in stream_runner.stream(query):\n            yield chunk\n    \n    return StreamingResponse(generate(), media_type=\"text/plain\")\n\n# Client receives chunks in real-time\n# Each chunk is validated before being sent",
          "input": "Setting up streaming endpoint...",
          "output": "✅ Streaming endpoint ready"
        }
      ]
    },
    {
      "type": "section",
      "title": "Async - Complete Guide",
      "subsections": [
        {
          "title": "Step 1: Create AsyncGuard",
          "type": "code",
          "code": "from wall_library import AsyncGuard, OnFailAction\nfrom wall_library.validator_base import Validator, register_validator\n\n# Create async guard\nasync_guard = AsyncGuard()\n\n# Add validators (same as regular guard)\nasync_guard.use((SafetyValidator, {}, OnFailAction.EXCEPTION))\n\n# AsyncGuard works exactly like WallGuard but with async methods",
          "input": "Creating AsyncGuard...",
          "output": "✅ AsyncGuard created"
        },
        {
          "title": "Step 2: Async Validation",
          "type": "code",
          "code": "import asyncio\n\n# Async validation\nasync def validate_response(response: str):\n    outcome = await async_guard.async_validate(response)\n    \n    if outcome.validation_passed:\n        return outcome.validated_output\n    else:\n        raise ValueError(f\"Validation failed: {outcome.error_messages}\")\n\n# Use in async function\nasync def process_query(query: str):\n    # Get LLM response (async)\n    response = await async_llm_api(query)\n    \n    # Validate (async)\n    validated = await validate_response(response)\n    \n    return validated\n\n# Run async function\nresult = asyncio.run(process_query(\"What are diabetes symptoms?\"))",
          "input": "Running async validation...",
          "output": "✅ Async validation completed"
        },
        {
          "title": "Step 3: Async Guard with LLM",
          "type": "code",
          "code": "from openai import AsyncOpenAI\n\n# Create async OpenAI client\nclient = AsyncOpenAI(api_key=\"your-api-key\")\n\nasync def async_llm_api(prompt: str, **kwargs):\n    \"\"\"Async LLM API call.\"\"\"\n    response = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        **kwargs\n    )\n    return response.choices[0].message.content\n\n# Use async guard with LLM\nasync def process_with_async_guard(query: str):\n    # Guard automatically calls LLM and validates (async)\n    raw, validated, outcome = await async_guard(\n        llm_api=async_llm_api,\n        prompt=query\n    )\n    \n    if outcome.validation_passed:\n        return validated\n    else:\n        raise ValueError(\"Validation failed\")\n\n# Process multiple queries concurrently\nasync def process_multiple(queries: list):\n    # Process all queries concurrently\n    tasks = [process_with_async_guard(q) for q in queries]\n    results = await asyncio.gather(*tasks)\n    return results\n\n# Run\nqueries = [\"Query 1\", \"Query 2\", \"Query 3\"]\nresults = asyncio.run(process_multiple(queries))",
          "input": "Running async guard with LLM...",
          "output": "✅ All queries processed concurrently"
        }
      ]
    },
    {
      "type": "section",
      "title": "Execution Modes",
      "subsections": [
        {
          "title": "Runner (Synchronous)",
          "type": "text",
          "content": "**Runner** is the default synchronous execution mode.\n\n- **Blocking**: Waits for LLM response\n- **Simple**: Easiest to use\n- **Use when**: Simple scripts, synchronous applications"
        },
        {
          "title": "AsyncRunner (Asynchronous)",
          "type": "text",
          "content": "**AsyncRunner** executes LLM calls asynchronously.\n\n- **Non-blocking**: Doesn't block while waiting\n- **Concurrent**: Can handle multiple requests\n- **Use when**: Web applications, APIs, concurrent processing"
        },
        {
          "title": "StreamRunner (Streaming)",
          "type": "text",
          "content": "**StreamRunner** processes responses chunk-by-chunk.\n\n- **Real-time**: Yields chunks as they arrive\n- **Validated chunks**: Each chunk is validated\n- **Use when**: User-facing applications, real-time responses"
        },
        {
          "title": "AsyncStreamRunner (Async Streaming)",
          "type": "text",
          "content": "**AsyncStreamRunner** combines async and streaming.\n\n- **Non-blocking streaming**: Async + streaming\n- **Concurrent streaming**: Multiple streams concurrently\n- **Use when**: High-throughput streaming applications"
        }
      ]
    },
    {
      "type": "section",
      "title": "Best Practices",
      "subsections": [
        {
          "title": "Recommendations",
          "type": "list",
          "items": [
            "**Use async for web apps**: Use AsyncGuard in FastAPI, Flask async, etc.",
            "**Use streaming for UX**: Use StreamRunner for better user experience",
            "**Handle errors properly**: Async errors need proper handling",
            "**Validate chunks**: StreamRunner validates each chunk",
            "**Concurrent processing**: Use async for handling multiple requests",
            "**Resource management**: Be mindful of concurrent request limits"
          ]
        }
      ]
    }
  ]
}
