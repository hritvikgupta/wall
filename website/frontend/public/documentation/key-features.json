{
  "id": "key-features",
  "title": "Key Features",
  "description": "Comprehensive overview of all Wall Library features with detailed explanations",
  "content": [
    {
      "type": "text",
      "content": "Wall Library provides a comprehensive set of features for LLM validation and management. Each feature is designed to work independently or together to create a complete validation pipeline for your LLM applications."
    },
    {
      "type": "section",
      "title": "üõ°Ô∏è Wall Guard - Core Validation Engine",
      "subsections": [
        {
          "title": "What It Does",
          "type": "text",
          "content": "Wall Guard is the heart of Wall Library. It's a multi-validator system that chains together multiple validation rules to check LLM inputs and outputs. Think of it as a firewall that sits between your LLM and your application, ensuring only safe, validated responses pass through."
        },
        {
          "title": "Key Capabilities",
          "type": "list",
          "items": [
            "**Sequential Validator Execution**: Runs validators one after another in a pipeline",
            "**OnFailActions**: Defines what happens when validation fails (EXCEPTION, REASK, FIX, FILTER, REFRAIN, NOOP)",
            "**Input & Output Validation**: Can validate both user prompts and LLM responses",
            "**LLM Integration**: Automatically calls LLM and validates responses",
            "**Re-asking Mechanism**: Automatically retries with feedback if validation fails",
            "**Configurable Execution**: Control number of retries, logging, and telemetry"
          ]
        },
        {
          "title": "Use Cases",
          "type": "list",
          "items": [
            "Safety validation (blocking dangerous content)",
            "Quality assurance (ensuring responses meet standards)",
            "Format validation (ensuring structured outputs)",
            "Domain-specific validation (healthcare, legal, finance rules)"
          ]
        },
        {
          "title": "Example",
          "type": "code",
          "code": "from wall_library import WallGuard, OnFailAction\n\n# Create guard with validators\nguard = WallGuard().use(\n    (SafetyValidator, {}, OnFailAction.EXCEPTION)\n).use(\n    (LengthValidator, {\"min_length\": 50}, OnFailAction.EXCEPTION)\n)\n\n# Validate response\nresult = guard.validate(llm_response)\nif result.validation_passed:\n    return result.validated_output",
          "input": "Using Wall Guard...",
          "output": "‚úÖ Response validated successfully"
        }
      ]
    },
    {
      "type": "section",
      "title": "üß† NLP Context Manager - Domain Boundary Enforcement",
      "subsections": [
        {
          "title": "What It Does",
          "type": "text",
          "content": "Context Manager ensures LLM responses stay within your approved domain boundaries using NLP techniques. It uses keyword matching and semantic similarity to determine if a response is within your approved context (e.g., healthcare, legal, finance)."
        },
        {
          "title": "Key Capabilities",
          "type": "list",
          "items": [
            "**Keyword Matching**: Fast exact/fuzzy keyword matching for quick filtering",
            "**Semantic Similarity**: Embedding-based similarity using cosine similarity",
            "**File Loading**: Load contexts from TXT, JSON, or CSV files",
            "**Threshold-Based Filtering**: Configurable similarity thresholds (0.0 to 1.0)",
            "**Batch Processing**: Check multiple responses efficiently"
          ]
        },
        {
          "title": "Use Cases",
          "type": "list",
          "items": [
            "Domain restriction (healthcare only, legal only, etc.)",
            "Topic filtering (prevent off-topic responses)",
            "Content boundary enforcement",
            "Multi-domain applications (different contexts for different queries)"
          ]
        },
        {
          "title": "Example",
          "type": "code",
          "code": "from wall_library.nlp import ContextManager\n\n# Create context manager\ncontext_manager = ContextManager()\ncontext_manager.add_keywords([\"healthcare\", \"medical\", \"doctor\"])\ncontext_manager.add_string_list([\"General health information\", \"Symptom description\"])\n\n# Check if response is in context\nis_valid = context_manager.check_context(response, threshold=0.7)",
          "input": "Checking context...",
          "output": "‚úÖ Response is within approved context"
        }
      ]
    },
    {
      "type": "section",
      "title": "üîç RAG Integration - Knowledge Grounding",
      "subsections": [
        {
          "title": "What It Does",
          "type": "text",
          "content": "RAG (Retrieval-Augmented Generation) retrieves relevant knowledge from a vector database to ground LLM responses in verified information. This reduces hallucinations and ensures responses are based on your knowledge base rather than the LLM's training data."
        },
        {
          "title": "Key Capabilities",
          "type": "list",
          "items": [
            "**Vector Similarity Search**: Uses ChromaDB for fast vector search",
            "**Top-K Retrieval**: Retrieves top-k most relevant contexts",
            "**Relevance Scoring**: Scores retrieved contexts by relevance",
            "**Metadata Filtering**: Filter results by metadata (topic, domain, etc.)",
            "**Hybrid Search**: Combines semantic and keyword search",
            "**Embedding Services**: Supports sentence-transformers and OpenAI embeddings"
          ]
        },
        {
          "title": "Use Cases",
          "type": "list",
          "items": [
            "Knowledge-grounded responses (reduce hallucinations)",
            "Domain-specific knowledge bases (healthcare, legal, etc.)",
            "Up-to-date information (update knowledge without retraining)",
            "Traceability (know which sources were used)"
          ]
        },
        {
          "title": "Example",
          "type": "code",
          "code": "from wall_library.rag import RAGRetriever, ChromaDBClient, EmbeddingService\n\n# Setup RAG\nchromadb = ChromaDBClient(collection_name=\"knowledge_base\")\nembedding = EmbeddingService(provider=\"sentence-transformers\")\nrag = RAGRetriever(chromadb_client=chromadb, embedding_service=embedding, top_k=5)\n\n# Add knowledge base\nchromadb.add_qa_pairs(questions, answers, metadata)\n\n# Retrieve context\nretrieved = rag.retrieve(\"What are diabetes symptoms?\", top_k=3)\ncontext = retrieved[0]['document']  # Use most relevant context",
          "input": "Retrieving knowledge...",
          "output": "‚úÖ Retrieved 3 relevant contexts"
        }
      ]
    },
    {
      "type": "section",
      "title": "üìä Response Scoring - Quality Metrics",
      "subsections": [
        {
          "title": "What It Does",
          "type": "text",
          "content": "Response Scorer evaluates LLM responses against expected outputs using multiple metrics. It provides quantitative measures of response quality, helping you understand how well your LLM is performing."
        },
        {
          "title": "Key Capabilities",
          "type": "list",
          "items": [
            "**Multiple Metrics**: ROUGE, BLEU, Cosine Similarity, Semantic Similarity",
            "**Aggregated Scoring**: Combine multiple metrics into a single score",
            "**Weighted Scoring**: Assign weights to different metrics",
            "**Threshold-Based Evaluation**: Pass/fail based on score thresholds",
            "**Custom Metrics**: Define your own scoring functions"
          ]
        },
        {
          "title": "Available Metrics",
          "type": "list",
          "items": [
            "**ROUGEMetric**: Measures n-gram overlap (good for summarization)",
            "**BLEUMetric**: Measures precision of n-grams (good for translation)",
            "**CosineSimilarityMetric**: Vector cosine similarity",
            "**SemanticSimilarityMetric**: Semantic meaning similarity using embeddings",
            "**CustomMetric**: Your own scoring function"
          ]
        },
        {
          "title": "Use Cases",
          "type": "list",
          "items": [
            "Quality assessment (how good are the responses?)",
            "Performance evaluation (tracking improvements over time)",
            "A/B testing (comparing different models or prompts)",
            "Threshold-based filtering (only accept high-quality responses)"
          ]
        },
        {
          "title": "Example",
          "type": "code",
          "code": "from wall_library.scoring import ResponseScorer, ROUGEMetric, BLEUMetric\n\n# Create scorer\nscorer = ResponseScorer()\nscorer.metrics.append(ROUGEMetric())\nscorer.metrics.append(BLEUMetric())\n\n# Score response\nscores = scorer.score(response, expected_output)\naggregated = scorer.aggregate_score(scores)\nprint(f\"Quality score: {aggregated:.3f}\")",
          "input": "Scoring response...",
          "output": "Quality score: 0.850"
        }
      ]
    },
    {
      "type": "section",
      "title": "üìà LLM Monitoring - Tracking & Analytics",
      "subsections": [
        {
          "title": "What It Does",
          "type": "text",
          "content": "LLM Monitor tracks all LLM interactions for monitoring and analytics. It records inputs, outputs, latency, success rates, and metadata, providing comprehensive observability for your LLM applications."
        },
        {
          "title": "Key Capabilities",
          "type": "list",
          "items": [
            "**Interaction Tracking**: Records all LLM inputs and outputs",
            "**Latency Monitoring**: Tracks response times",
            "**Success/Failure Tracking**: Monitors validation pass/fail rates",
            "**Metadata Collection**: Stores custom metadata for each interaction",
            "**Statistics API**: Get aggregated statistics (total interactions, avg latency, success rate)",
            "**OpenTelemetry Integration**: Export metrics to OpenTelemetry for distributed tracing"
          ]
        },
        {
          "title": "Use Cases",
          "type": "list",
          "items": [
            "Production monitoring (track performance in real-time)",
            "Performance analysis (identify bottlenecks)",
            "Debugging (trace issues with specific interactions)",
            "Analytics and reporting (understand usage patterns)",
            "Compliance (audit trail of all LLM interactions)"
          ]
        },
        {
          "title": "Example",
          "type": "code",
          "code": "from wall_library.monitoring import LLMMonitor\n\n# Create monitor\nmonitor = LLMMonitor()\n\n# Track LLM call\nmonitor.track_call(\n    input_data=\"What are diabetes symptoms?\",\n    output=\"Common symptoms include...\",\n    metadata={\"model\": \"gpt-3.5-turbo\", \"domain\": \"healthcare\"},\n    latency=0.45\n)\n\n# Get statistics\nstats = monitor.get_stats()\nprint(f\"Total interactions: {stats['total_interactions']}\")\nprint(f\"Avg latency: {stats['metrics']['avg_latency']}s\")",
          "input": "Tracking LLM call...",
          "output": "Total interactions: 1\nAvg latency: 0.45s"
        }
      ]
    },
    {
      "type": "section",
      "title": "üìù Comprehensive Logging - Automatic Operation Logging",
      "subsections": [
        {
          "title": "What It Does",
          "type": "text",
          "content": "Wall Logger provides automatic logging of all Wall Library operations. It logs validation operations, RAG retrievals, scoring operations, LLM calls, and monitoring events, making debugging and compliance easy."
        },
        {
          "title": "Key Capabilities",
          "type": "list",
          "items": [
            "**Multiple Log Scopes**: VALIDATION, RAG, SCORING, LLM_CALLS, MONITORING, ALL",
            "**Multiple Outputs**: File, console, or both",
            "**Multiple Formats**: JSON (structured), human-readable, or both",
            "**Automatic Logging**: Set logger once, all operations are logged automatically",
            "**Configurable Levels**: DEBUG, INFO, WARNING, ERROR",
            "**Log Rotation**: Automatic log file rotation (configurable size and backup count)"
          ]
        },
        {
          "title": "Log Scopes",
          "type": "list",
          "items": [
            "**VALIDATION**: Log all validation operations (pass/fail, errors)",
            "**RAG**: Log RAG retrievals (queries, retrieved docs, scores)",
            "**SCORING**: Log scoring operations (metrics, scores)",
            "**LLM_CALLS**: Log LLM interactions (inputs, outputs, latency)",
            "**MONITORING**: Log monitoring events",
            "**ALL**: Log everything (recommended for production)"
          ]
        },
        {
          "title": "Use Cases",
          "type": "list",
          "items": [
            "Debugging (trace issues with detailed logs)",
            "Compliance (audit trail for regulatory requirements)",
            "Analytics (analyze logs for patterns)",
            "Integration with log aggregators (ELK, Splunk, etc.)"
          ]
        },
        {
          "title": "Example",
          "type": "code",
          "code": "from wall_library.logging import WallLogger, LogScope\nimport os\n\n# Create logger\nlogger = WallLogger(\n    level=\"INFO\",\n    scopes=[LogScope.ALL.value],\n    output=\"both\",  # File + console\n    format=\"both\",  # JSON + human-readable\n    log_file=\"wall_library.log\"\n)\n\n# Set logger on components\nguard.set_logger(logger)\nrag_retriever.set_logger(logger)\nscorer.set_logger(logger)\nmonitor.set_logger(logger)\n\n# Now all operations are automatically logged!",
          "input": "Setting up logging...",
          "output": "‚úÖ Logger configured. All operations will be logged."
        }
      ]
    },
    {
      "type": "section",
      "title": "üìä Visualization - Visual Analytics",
      "subsections": [
        {
          "title": "What It Does",
          "type": "text",
          "content": "Visualization module provides comprehensive visual analytics including 3D graphs, word clouds, context boundaries, and monitoring dashboards. It helps you understand your LLM application's performance through visual representations."
        },
        {
          "title": "Key Capabilities",
          "type": "list",
          "items": [
            "**Score Visualizations**: Bar charts showing metric scores with color coding",
            "**Context Boundaries**: Pie charts and bar charts showing inside/outside context analysis",
            "**Keyword Analysis**: Frequency and distribution charts",
            "**Word Clouds**: Text visualization with word frequency",
            "**3D Embeddings**: Interactive 3D scatter plots showing embedding space (IMPORTANT)",
            "**3D Scores**: Multi-metric 3D visualization (IMPORTANT)",
            "**Validation Results**: Timeline and performance charts",
            "**RAG Retrieval Analysis**: Score and distance distributions",
            "**Monitoring Dashboards**: Comprehensive analytics dashboard"
          ]
        },
        {
          "title": "Visualization Types",
          "type": "list",
          "items": [
            "**Score Visualization**: Bar charts with color coding (green/yellow/red based on thresholds)",
            "**Context Boundaries**: Pie charts showing % inside/outside, bar charts with similarity scores",
            "**Keywords**: Frequency charts showing keyword distribution",
            "**Word Clouds**: Visual word frequency representation",
            "**3D Embeddings**: Interactive Plotly 3D scatter plots (can rotate, zoom, hover)",
            "**3D Scores**: 3D plots with multiple metrics on x/y/z axes",
            "**Monitoring Dashboard**: Multi-panel dashboard with metrics, charts, and statistics"
          ]
        },
        {
          "title": "Use Cases",
          "type": "list",
          "items": [
            "Data analysis (understand patterns in responses)",
            "Presentation (show results to stakeholders)",
            "Debugging (visualize where issues occur)",
            "Reporting (generate visual reports)"
          ]
        },
        {
          "title": "Example",
          "type": "code",
          "code": "from wall_library.visualization import WallVisualizer\n\n# Create visualizer\nviz = WallVisualizer(output_dir=\"visualizations\")\n\n# Visualize scores\nscores = {\"CosineSimilarity\": 0.85, \"ROUGEMetric\": 0.72}\nviz.visualize_scores(scores, title=\"Response Quality\")\n\n# Visualize context boundaries\nviz.visualize_context_boundaries(responses, context_manager)\n\n# Generate word cloud\nviz.visualize_wordcloud(text, title=\"Domain Word Cloud\")\n\n# 3D visualizations\nviz.visualize_3d_embeddings(embeddings, labels)\nviz.visualize_3d_scores(scores_data)",
          "input": "Generating visualizations...",
          "output": "‚úÖ Generated: scores.png, context_boundaries.png, wordcloud.png, 3d_embeddings.html, 3d_scores.html"
        }
      ]
    },
    {
      "type": "section",
      "title": "üìê Structured Output - Schema Validation",
      "subsections": [
        {
          "title": "What It Does",
          "type": "text",
          "content": "Schema Systems ensure LLM outputs match expected structure using Pydantic, RAIL, or JSON Schema. This guarantees that LLM responses are in the exact format your application expects."
        },
        {
          "title": "Key Capabilities",
          "type": "list",
          "items": [
            "**Pydantic Schema**: Use Pydantic models to define structure",
            "**RAIL Schema**: XML-based schema definition with validators",
            "**JSON Schema**: Standard JSON schema validation",
            "**Type Validation**: Ensures correct data types",
            "**Field Validation**: Validates individual fields",
            "**Nested Models**: Supports complex nested structures"
          ]
        },
        {
          "title": "Schema Types",
          "type": "list",
          "items": [
            "**Pydantic**: Python-first, type-safe, with automatic validation",
            "**RAIL**: XML-based, supports validators per field, on-fail actions",
            "**JSON Schema**: Standard format, widely supported"
          ]
        },
        {
          "title": "Use Cases",
          "type": "list",
          "items": [
            "Structured data extraction (extract entities from text)",
            "API responses (ensure consistent format)",
            "Data pipelines (guarantee structure for downstream processing)",
            "Form filling (extract form data from unstructured text)"
          ]
        },
        {
          "title": "Example",
          "type": "code",
          "code": "from wall_library import WallGuard\nfrom pydantic import BaseModel, Field\n\nclass PatientInfo(BaseModel):\n    condition: str = Field(description=\"Medical condition\")\n    symptoms: list[str] = Field(description=\"List of symptoms\")\n    severity: str = Field(description=\"Severity level\")\n\n# Create guard with Pydantic schema\nguard = WallGuard.for_pydantic(output_class=PatientInfo)\n\n# LLM output is automatically validated against schema\nraw, validated, outcome = guard(llm_api=llm_call, prompt=\"Extract patient info\")\nif outcome.validation_passed:\n    print(f\"Condition: {validated['condition']}\")",
          "input": "Validating structured output...",
          "output": "‚úÖ Validated: Condition: diabetes, Symptoms: ['thirst', 'urination']"
        }
      ]
    },
    {
      "type": "section",
      "title": "üîó Framework Integration - LangChain/LangGraph",
      "subsections": [
        {
          "title": "What It Does",
          "type": "text",
          "content": "Framework Wrappers make Wall Library guards usable in LangChain and LangGraph workflows. This allows you to use Wall Library validation in existing LangChain/LangGraph applications without rewriting your code."
        },
        {
          "title": "Key Capabilities",
          "type": "list",
          "items": [
            "**LangChain Integration**: Convert guards to LangChain Runnables",
            "**LangGraph Integration**: Create nodes with guard validation",
            "**Seamless Integration**: Works with existing LangChain/LangGraph code",
            "**Chain Support**: Use guards in LangChain chains",
            "**Agent Support**: Use guards in LangChain agents"
          ]
        },
        {
          "title": "Supported Frameworks",
          "type": "list",
          "items": [
            "**LangChain**: Full integration with chains, agents, and tools",
            "**LangGraph**: Node creation with guard validation",
            "**LlamaIndex**: Guardrails chat engine and query engine",
            "**Databricks MLflow**: MLflow instrumentation and tracking"
          ]
        },
        {
          "title": "Use Cases",
          "type": "list",
          "items": [
            "Existing LangChain applications (add validation without rewriting)",
            "LangGraph workflows (validate at specific nodes)",
            "Multi-step chains (validate each step)",
            "Agent applications (validate agent responses)"
          ]
        },
        {
          "title": "Example",
          "type": "code",
          "code": "from wall_library import WallGuard\nfrom wall_library.wrappers import LangChainWrapper\n\n# Create guard\nguard = WallGuard().use((SafetyValidator, {}, OnFailAction.EXCEPTION))\n\n# Convert to LangChain runnable\nwrapper = LangChainWrapper(guard)\nrunnable = wrapper.to_runnable()\n\n# Use in LangChain chain\nresult = runnable.invoke({\"prompt\": \"What are diabetes symptoms?\", \"llm_api\": llm_api_call})",
          "input": "Integrating with LangChain...",
          "output": "‚úÖ Guard integrated as LangChain Runnable"
        }
      ]
    },
    {
      "type": "section",
      "title": "‚ö° Streaming & Async - Modern Execution Modes",
      "subsections": [
        {
          "title": "What It Does",
          "type": "text",
          "content": "Wall Library supports both streaming and asynchronous operations, allowing you to build modern, responsive applications that handle LLM interactions efficiently."
        },
        {
          "title": "Key Capabilities",
          "type": "list",
          "items": [
            "**Streaming**: Process LLM responses chunk-by-chunk as they arrive",
            "**Async Support**: Non-blocking async/await operations",
            "**Stream Validation**: Validate chunks as they stream",
            "**Async Validation**: Validate responses asynchronously",
            "**Concurrent Processing**: Handle multiple requests simultaneously"
          ]
        },
        {
          "title": "Execution Modes",
          "type": "list",
          "items": [
            "**Runner**: Synchronous execution (blocking)",
            "**AsyncRunner**: Asynchronous execution (non-blocking)",
            "**StreamRunner**: Streaming execution (chunk-by-chunk)",
            "**AsyncStreamRunner**: Async streaming (non-blocking chunks)"
          ]
        },
        {
          "title": "Use Cases",
          "type": "list",
          "items": [
            "Web applications (async for handling multiple requests)",
            "Streaming applications (real-time response display)",
            "High-throughput systems (concurrent processing)",
            "User-facing applications (responsive UI with streaming)"
          ]
        },
        {
          "title": "Example",
          "type": "code",
          "code": "from wall_library import AsyncGuard\nfrom wall_library.run import StreamRunner\n\n# Async validation\nasync_guard = AsyncGuard()\nasync_guard.use((SafetyValidator, {}, OnFailAction.EXCEPTION))\noutcome = await async_guard.async_validate(response)\n\n# Streaming\nstream_runner = StreamRunner(api=llm_api_stream, validation_map=guard.validator_map)\nfor chunk in stream_runner.stream(\"What are diabetes symptoms?\"):\n    print(chunk, end=\"\", flush=True)",
          "input": "Running async/streaming...",
          "output": "‚úÖ Async validation completed\n‚úÖ Streaming response received"
        }
      ]
    },
    {
      "type": "section",
      "title": "Putting It All Together",
      "subsections": [
        {
          "title": "Complete Pipeline",
          "type": "text",
          "content": "All these features work together to create a complete validation pipeline:\n\n1. **Wall Guard** validates inputs and outputs\n2. **Context Manager** ensures responses stay in domain\n3. **RAG Retriever** grounds responses in knowledge\n4. **Response Scorer** evaluates quality\n5. **LLM Monitor** tracks everything\n6. **Wall Logger** logs all operations\n7. **Visualization** provides insights\n8. **Schema Systems** ensure structure\n9. **Framework Integration** works with your stack\n10. **Streaming/Async** handles modern workloads\n\nEach feature can be used independently or combined for maximum protection and observability."
        },
        {
          "title": "Getting Started",
          "type": "text",
          "content": "Start with **Wall Guard** and **Validators** for basic validation. Then add **Context Manager** for domain filtering, **RAG Retriever** for knowledge grounding, and **Response Scorer** for quality assessment. Add **LLM Monitor** and **Wall Logger** for production observability. Use **Visualization** to understand your system's performance."
        }
      ]
    }
  ]
}
