{
  "id": "wall-guard",
  "title": "Wall Guard - Core Validation Engine",
  "description": "The heart of Wall Library - multi-validator validation system for LLM inputs and outputs",
  "content": [
    {
      "type": "text",
      "content": "**Wall Guard** is the core validation engine of Wall Library. It chains multiple validators together and executes them sequentially to validate LLM inputs and outputs. Think of it as a firewall that sits between your LLM and your application, ensuring only safe, validated responses pass through."
    },
    {
      "type": "section",
      "title": "Understanding Wall Guard",
      "subsections": [
        {
          "title": "What is Wall Guard?",
          "type": "text",
          "content": "Wall Guard is a validation pipeline that:\n\n1. **Accepts LLM outputs** (or inputs) for validation\n2. **Runs validators sequentially** - each validator checks the content against specific rules\n3. **Applies OnFailActions** - determines what happens when validation fails\n4. **Returns ValidationOutcome** - contains validated output, error messages, and metadata\n\nWall Guard is designed to be **chainable** - you can add multiple validators using the `.use()` method, and they execute in order."
        },
        {
          "title": "Key Concepts",
          "type": "list",
          "items": [
            "**Validators**: Rules that check if content meets specific criteria (safety, length, format, etc.)",
            "**OnFailActions**: What to do when validation fails (EXCEPTION, FILTER, REASK, FIX, etc.)",
            "**ValidationOutcome**: Result object containing validated output, pass/fail status, and error details",
            "**Chaining**: Adding multiple validators using `.use()` method",
            "**Input/Output validation**: Can validate both LLM inputs and outputs"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "Basic Usage - Step by Step",
      "subsections": [
        {
          "title": "Step 1: Import Required Modules",
          "type": "code",
          "code": "from wall_library import WallGuard, OnFailAction\nfrom wall_library.validator_base import Validator, register_validator\nfrom wall_library.classes.validation.validation_result import PassResult, FailResult",
          "input": "Importing modules...",
          "output": "✅ Modules imported successfully"
        },
        {
          "title": "Step 2: Create a Custom Validator",
          "type": "code",
          "code": "# Create a custom validator for minimum length\n@register_validator(\"min_length\")\nclass MinLengthValidator(Validator):\n    def __init__(self, min_length: int = 10, **kwargs):\n        # require_rc=False means we don't need a .wallrc config file\n        super().__init__(require_rc=False, **kwargs)\n        self.min_length = min_length\n    \n    def _validate(self, value: str, metadata: dict):\n        \"\"\"Validate that value meets minimum length requirement.\"\"\"\n        if len(value) < self.min_length:\n            return FailResult(\n                error_message=f\"Too short: {len(value)} < {self.min_length}\",\n                metadata=metadata\n            )\n        return PassResult(metadata=metadata)",
          "input": "Creating validator...",
          "output": "✅ Validator 'min_length' registered successfully"
        },
        {
          "title": "Step 3: Create and Configure Guard",
          "type": "code",
          "code": "# Create a guard instance\nguard = WallGuard()\n\n# Add validator to guard\n# Format: (ValidatorClass, {constructor_params}, OnFailAction)\nguard = guard.use(\n    (MinLengthValidator, {\"min_length\": 10}, OnFailAction.EXCEPTION)\n)\n\n# You can chain multiple validators\nguard = guard.use(\n    (AnotherValidator, {\"param\": \"value\"}, OnFailAction.FILTER)\n)",
          "input": "Configuring guard...",
          "output": "✅ Guard configured with validators"
        },
        {
          "title": "Step 4: Validate Content",
          "type": "code",
          "code": "# Validate a string\nresult = guard.validate(\"Hello World!\")\n\n# Check if validation passed\nif result.validation_passed:\n    print(f\"✅ Validated: {result.validated_output}\")\nelse:\n    print(f\"❌ Validation failed: {result.error_messages}\")\n    print(f\"Raw output: {result.raw_output}\")",
          "input": "Validating content...",
          "output": "✅ Validated: Hello World!"
        }
      ]
    },
    {
      "type": "section",
      "title": "Complete Example - Healthcare Safety Guard",
      "subsections": [
        {
          "title": "Full Implementation",
          "type": "code",
          "code": "from wall_library import WallGuard, OnFailAction\nfrom wall_library.validator_base import Validator, register_validator\nfrom wall_library.classes.validation.validation_result import PassResult, FailResult\n\n# Define restricted terms for healthcare\nRESTRICTED_TERMS = [\n    \"guaranteed cure\", \"miracle treatment\", \"instant relief\",\n    \"100% effective\", \"bypass doctor\", \"self-diagnose\"\n]\n\n# Create safety validator\n@register_validator(\"healthcare_safety\")\nclass HealthcareSafetyValidator(Validator):\n    def __init__(self, restricted_terms: list = None, **kwargs):\n        super().__init__(require_rc=False, **kwargs)\n        self.restricted_terms = restricted_terms or RESTRICTED_TERMS\n    \n    def _validate(self, value: str, metadata: dict):\n        if not isinstance(value, str):\n            return FailResult(\n                error_message=\"Response must be a string\",\n                metadata=metadata\n            )\n        \n        value_lower = value.lower()\n        found_restricted = []\n        \n        for term in self.restricted_terms:\n            if term.lower() in value_lower:\n                found_restricted.append(term)\n        \n        if found_restricted:\n            return FailResult(\n                error_message=f\"Response contains restricted terms: {', '.join(found_restricted)}\",\n                metadata={**metadata, \"restricted_terms\": found_restricted}\n            )\n        \n        return PassResult(metadata=metadata)\n\n# Create length validator\n@register_validator(\"healthcare_length\")\nclass HealthcareLengthValidator(Validator):\n    def __init__(self, min_length: int = 50, max_length: int = 2000, **kwargs):\n        super().__init__(require_rc=False, **kwargs)\n        self.min_length = min_length\n        self.max_length = max_length\n    \n    def _validate(self, value: str, metadata: dict):\n        length = len(value)\n        if length < self.min_length:\n            return FailResult(\n                error_message=f\"Too short: {length} < {self.min_length}\",\n                metadata=metadata\n            )\n        if length > self.max_length:\n            return FailResult(\n                error_message=f\"Too long: {length} > {self.max_length}\",\n                metadata=metadata\n            )\n        return PassResult(metadata=metadata)\n\n# Create guard with multiple validators\nguard = WallGuard().use(\n    (HealthcareSafetyValidator, {\"restricted_terms\": RESTRICTED_TERMS}, OnFailAction.EXCEPTION)\n).use(\n    (HealthcareLengthValidator, {\"min_length\": 50, \"max_length\": 2000}, OnFailAction.EXCEPTION)\n)\n\n# Test with valid response\nvalid_response = \"Common symptoms of diabetes include increased thirst and frequent urination. Consult a healthcare provider for proper diagnosis.\"\nresult = guard.validate(valid_response)\nprint(f\"Validation passed: {result.validation_passed}\")\nprint(f\"Output: {result.validated_output}\")\n\n# Test with invalid response (contains restricted term)\ntry:\n    invalid_response = \"This is a guaranteed cure for diabetes!\"\n    result = guard.validate(invalid_response)\nexcept Exception as e:\n    print(f\"❌ Blocked: {e}\")",
          "input": "Running healthcare guard...",
          "output": "Validation passed: True\nOutput: Common symptoms of diabetes...\n❌ Blocked: Response contains restricted terms: guaranteed cure"
        }
      ]
    },
    {
      "type": "section",
      "title": "Using Guard with LLM API",
      "subsections": [
        {
          "title": "Integration with OpenAI",
          "type": "code",
          "code": "from openai import OpenAI\nfrom wall_library import WallGuard, OnFailAction\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=\"your-api-key\")\n\ndef llm_api_call(prompt: str, **kwargs):\n    \"\"\"Wrapper function for OpenAI API calls.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        **kwargs\n    )\n    return response.choices[0].message.content\n\n# Create guard (assuming validators are already defined)\nguard = WallGuard().use(\n    (SafetyValidator, {}, OnFailAction.EXCEPTION)\n)\n\n# Guard automatically calls LLM and validates response\n# Returns: (raw_output, validated_output, outcome)\nraw_output, validated_output, outcome = guard(\n    llm_api=llm_api_call,\n    prompt=\"What are diabetes symptoms?\"\n)\n\nprint(f\"Raw LLM output: {raw_output}\")\nprint(f\"Validated output: {validated_output}\")\nprint(f\"Validation passed: {outcome.validation_passed}\")\n\n# If validation failed, validated_output will be None\nif not outcome.validation_passed:\n    print(f\"Errors: {outcome.error_messages}\")",
          "input": "Calling LLM with guard...",
          "output": "Raw LLM output: Common symptoms of diabetes include...\nValidated output: Common symptoms of diabetes include...\nValidation passed: True"
        },
        {
          "title": "Understanding the Return Values",
          "type": "text",
          "content": "When you call `guard(llm_api=..., prompt=...)`, it returns a tuple:\n\n1. **raw_output**: The original, unvalidated response from the LLM\n2. **validated_output**: The validated response (None if validation failed)\n3. **outcome**: A ValidationOutcome object containing:\n   - `validation_passed`: Boolean indicating if validation passed\n   - `error_messages`: List of error messages if validation failed\n   - `error_spans`: Locations of errors in the text\n   - `metadata`: Additional validation information"
        }
      ]
    },
    {
      "type": "section",
      "title": "Advanced Features",
      "subsections": [
        {
          "title": "Configuring Guard Settings",
          "type": "code",
          "code": "# Create guard with configuration\nguard = WallGuard(\n    num_reasks=3,  # Maximum number of re-asks if validation fails\n    name=\"my_guard\",  # Name for logging/tracking\n    logger=my_logger  # Optional WallLogger instance\n)\n\n# Or configure after creation\nguard.configure(\n    num_reasks=5,\n    tracer=my_tracer  # For OpenTelemetry\n)\n\n# Set logger separately\nguard.set_logger(my_logger)",
          "input": "Configuring guard...",
          "output": "✅ Guard configured"
        },
        {
          "title": "Input vs Output Validation",
          "type": "code",
          "code": "# Validate input (user prompt)\nguard.use(\n    (InputValidator, {}, OnFailAction.EXCEPTION),\n    on=\"input\"  # Apply to input\n)\n\n# Validate output (LLM response)\nguard.use(\n    (OutputValidator, {}, OnFailAction.EXCEPTION),\n    on=\"output\"  # Apply to output (default)\n)\n\n# You can have different validators for input and output\nguard.use((PromptSafetyValidator, {}, OnFailAction.EXCEPTION), on=\"input\")\nguard.use((ResponseSafetyValidator, {}, OnFailAction.EXCEPTION), on=\"output\")",
          "input": "Setting up input/output validation...",
          "output": "✅ Validators configured for input and output"
        },
        {
          "title": "Using Multiple Validators",
          "type": "code",
          "code": "# Method 1: Chain with .use()\nguard = WallGuard()\\\n    .use((SafetyValidator, {}, OnFailAction.EXCEPTION))\\\n    .use((LengthValidator, {\"min_length\": 50}, OnFailAction.EXCEPTION))\\\n    .use((QualityValidator, {}, OnFailAction.REASK))\n\n# Method 2: Use .use_many() for multiple at once\nguard = WallGuard().use_many(\n    (SafetyValidator, {}, OnFailAction.EXCEPTION),\n    (LengthValidator, {\"min_length\": 50}, OnFailAction.EXCEPTION),\n    (QualityValidator, {}, OnFailAction.REASK)\n)",
          "input": "Adding multiple validators...",
          "output": "✅ Multiple validators added"
        }
      ]
    },
    {
      "type": "section",
      "title": "Working with ValidationOutcome",
      "subsections": [
        {
          "title": "Accessing Validation Results",
          "type": "code",
          "code": "# Validate and get outcome\noutcome = guard.validate(\"Some text\")\n\n# Check if validation passed\nif outcome.validation_passed:\n    # Access validated output\n    safe_output = outcome.validated_output\n    print(f\"✅ Safe output: {safe_output}\")\nelse:\n    # Access error information\n    print(f\"❌ Validation failed\")\n    print(f\"Raw output: {outcome.raw_output}\")\n    print(f\"Error messages: {outcome.error_messages}\")\n    \n    # Access error spans (locations in text)\n    if outcome.error_spans:\n        for span in outcome.error_spans:\n            print(f\"Error at position {span.start}-{span.end}\")\n    \n    # Access metadata\n    if outcome.metadata:\n        print(f\"Metadata: {outcome.metadata}\")",
          "input": "Processing validation outcome...",
          "output": "✅ Safe output: Some text"
        }
      ]
    },
    {
      "type": "section",
      "title": "Best Practices",
      "subsections": [
        {
          "title": "Recommended Patterns",
          "type": "list",
          "items": [
            "**Order validators by strictness**: Put strictest validators (EXCEPTION) first, lenient ones (REASK) last",
            "**Use appropriate OnFailActions**: EXCEPTION for safety, REASK for quality, FILTER for content moderation",
            "**Set up logging**: Use WallLogger to track all validations for debugging and compliance",
            "**Name your guards**: Use `name` parameter for better tracking in logs and monitoring",
            "**Validate both input and output**: Protect against prompt injection and unsafe outputs",
            "**Handle ValidationOutcome properly**: Always check `validation_passed` before using `validated_output`"
          ]
        },
        {
          "title": "Common Mistakes to Avoid",
          "type": "list",
          "items": [
            "❌ Not checking `validation_passed` before using `validated_output`",
            "❌ Using `raw_output` instead of `validated_output`",
            "❌ Not handling exceptions when using OnFailAction.EXCEPTION",
            "❌ Adding too many validators (can slow down validation)",
            "❌ Not setting up logging (makes debugging difficult)",
            "❌ Using wrong OnFailAction for the use case"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "Real-World Example: Complete Healthcare Guard",
      "subsections": [
        {
          "title": "Production-Ready Implementation",
          "type": "code",
          "code": "from wall_library import WallGuard, OnFailAction, WallLogger, LogScope\nimport os\n\n# Setup logging\nlog_dir = os.path.join(os.getcwd(), \"logs\")\nos.makedirs(log_dir, exist_ok=True)\nlogger = WallLogger(\n    level=\"INFO\",\n    scopes=[LogScope.ALL.value],\n    output=\"file\",\n    log_file=os.path.join(log_dir, \"healthcare_guard.log\")\n)\n\n# Create comprehensive healthcare guard\nguard = WallGuard(\n    name=\"healthcare_guard\",\n    num_reasks=2,  # Allow 2 re-asks for quality improvement\n    logger=logger\n)\n\n# Add validators in order of strictness\nguard.use(\n    (HealthcareSafetyValidator, {\"restricted_terms\": RESTRICTED_TERMS}, OnFailAction.EXCEPTION)\n).use(\n    (HealthcareLengthValidator, {\"min_length\": 50, \"max_length\": 2000}, OnFailAction.EXCEPTION)\n).use(\n    (ContextValidator, {\"context_manager\": context_manager}, OnFailAction.EXCEPTION)\n)\n\n# Use guard in production\ndef process_healthcare_query(query: str):\n    try:\n        # Call LLM with guard\n        raw, validated, outcome = guard(\n            llm_api=llm_api_call,\n            prompt=query\n        )\n        \n        if outcome.validation_passed:\n            return validated\n        else:\n            # Log failure for monitoring\n            logger.warning(f\"Validation failed: {outcome.error_messages}\")\n            return None\n    except Exception as e:\n        # Handle exceptions from EXCEPTION OnFailAction\n        logger.error(f\"Guard blocked unsafe content: {e}\")\n        return None",
          "input": "Setting up production guard...",
          "output": "✅ Production-ready healthcare guard configured"
        }
      ]
    }
  ]
}
